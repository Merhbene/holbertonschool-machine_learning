{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6SmsAqFLAAq"
      },
      "source": [
        "# **Machine Learning Language Translation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t7YdJL1ordE"
      },
      "source": [
        "[Encoder And Decoder- Neural Machine Learning Language Translation Tutorial With Keras- Deep Learning](https://www.youtube.com/watch?v=f-JCCOHwx1c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJhR6jPppgC5"
      },
      "source": [
        "[Keras Blog](https://https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzTxVXJcqA1I"
      },
      "source": [
        "We will use a dataset of pairs of English sentences and their French translation, which you can download from [manythings.org/anki](https://manythings.org/anki). The file to download is called fra-eng.zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtOm_dP4I6v5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-6laNoHLceG"
      },
      "outputs": [],
      "source": [
        "batch_size = 64 # batch size for training\n",
        "epochs = 100 # Number of epochs to train for\n",
        "latent_dim = 256 # Latent dimentionality of the encoding space\n",
        "num_samples = 10000 # Number of samples to train on \n",
        "# path to the data txt file on disk\n",
        "data_path = \"/content/fra.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUxMnj1iMxdC"
      },
      "source": [
        "## Vectorize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIchCOuIMvuZ"
      },
      "outputs": [],
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    # we use 'tab' as the \"start sequence\" character for the targets, and '\\n' as \"end sequence\" character\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "          \n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL8KYJqAQPoz"
      },
      "outputs": [],
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts[50:60], target_texts[50:60]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr7Bc53rjBc4",
        "outputId": "dcf7c314-73e7-4144-cb48-9fe3b55fd246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['I see.',\n",
              "  'I try.',\n",
              "  'I won!',\n",
              "  'I won!',\n",
              "  'I won.',\n",
              "  'Oh no!',\n",
              "  'Relax.',\n",
              "  'Relax.',\n",
              "  'Relax.',\n",
              "  'Relax.'],\n",
              " ['\\tAha.\\n',\n",
              "  \"\\tJ'essaye.\\n\",\n",
              "  \"\\tJ'ai gagné !\\n\",\n",
              "  \"\\tJe l'ai emporté !\\n\",\n",
              "  '\\tJ’ai gagné.\\n',\n",
              "  '\\tOh non !\\n',\n",
              "  '\\tCalme-toi.\\n',\n",
              "  '\\tDétends-toi\\u202f!\\n',\n",
              "  '\\tDétendez-vous\\u202f!\\n',\n",
              "  '\\tRelaxe, Max\\u202f!\\n'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRPW43IeTQek"
      },
      "outputs": [],
      "source": [
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVMP8yetTvvt",
        "outputId": "6b4057d9-2818-4fc9-c036-f7339de99310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples:  10000\n",
            "Number of unique input tokens:  71\n",
            "Number of unique output tokens:  92\n",
            "Max sequence length for inputs:  15\n",
            "Max sequence length for outputs:  59\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of samples: \", len(input_texts))\n",
        "print(\"Number of unique input tokens: \", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens: \", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs: \", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs: \", max_decoder_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZK9TU7TUtuR"
      },
      "outputs": [],
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU6UKrvxWEoL"
      },
      "outputs": [],
      "source": [
        "input_token_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UQQt6sqaAYQ"
      },
      "source": [
        "## Turn the sentences into 3 Numpy arrays:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT1CyZznZmzl"
      },
      "source": [
        "\n",
        "\n",
        "* **encoder_input_data** is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
        "* **decoder_input_data** is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n",
        "* **decoder_target_data** is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bpi9daubWE5U"
      },
      "outputs": [],
      "source": [
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOr1-K0rUC7n",
        "outputId": "4a2139c9-c9d1-4ae7-ea04-754b39b42dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 15, 71), (10000, 59, 92), (10000, 59, 92))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oamYKPpca6Xw"
      },
      "outputs": [],
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1\n",
        "    encoder_input_data[i, t+1:, input_token_index[\" \"]] = 1\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character\n",
        "            decoder_input_data[i, t-1, target_token_index[char]] = 1\n",
        "    decoder_input_data[i, t+1:, input_token_index[\" \"]] = 1\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6pqaOhWepY8",
        "outputId": "463406de-16ba-4803-c597-16e5381b5187"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "encoder_input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2DcqTcLiUCF"
      },
      "source": [
        "## Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPdyxMAKeulk",
        "outputId": "9df2d1db-a4d5-4e02-8f10-672227d478b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQxtLHbuiFed"
      },
      "outputs": [],
      "source": [
        "# Define an input sequence and prcess it\n",
        "\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# we discard 'encoder_outputs' and only keep the states\n",
        "encoder_states = [state_h, state_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlIQk8_0jmRW",
        "outputId": "a9b87c5c-0017-4a34-efaf-f958699a0be4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "state_h.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_decoder_tokens"
      ],
      "metadata": {
        "id": "vw8Eif2Fe6gs",
        "outputId": "080e5a33-9a5d-4f24-94c8-29080c5668f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8tRL5C8jpVg"
      },
      "outputs": [],
      "source": [
        "# set up the decoder using 'encoder_states' as initial state\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O96Gb7wlTjf"
      },
      "source": [
        "We set up our decoder to return full output sequences, and to return internal states as well. We don't use the return states in the trainnig model, but we will use them in inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rq8ZS6WlSDz"
      },
      "outputs": [],
      "source": [
        "  decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ain4IhKmq9h"
      },
      "source": [
        "Define the model that will turn: 'encoder_input_data' & 'decoder_input_data' into 'decoder_target_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccYP_VsVlveR",
        "outputId": "48bcc244-df07-43e0-dd57-2b3772573908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "125/125 [==============================] - 58s 437ms/step - loss: 0.1107 - accuracy: 0.6931 - val_loss: 0.0343 - val_accuracy: 0.6638\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 53s 424ms/step - loss: 0.0600 - accuracy: 0.6986 - val_loss: 0.0784 - val_accuracy: 0.6638\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 52s 416ms/step - loss: 0.0576 - accuracy: 0.6986 - val_loss: 0.0428 - val_accuracy: 0.6638\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 52s 416ms/step - loss: 0.0514 - accuracy: 0.6986 - val_loss: 0.0514 - val_accuracy: 0.6638\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 53s 428ms/step - loss: 0.0406 - accuracy: 0.6986 - val_loss: 0.0262 - val_accuracy: 0.6638\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 53s 423ms/step - loss: 0.0325 - accuracy: 0.6986 - val_loss: 0.0179 - val_accuracy: 0.6638\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 54s 431ms/step - loss: 0.0271 - accuracy: 0.6986 - val_loss: 0.0365 - val_accuracy: 0.6638\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 53s 424ms/step - loss: 0.0244 - accuracy: 0.6986 - val_loss: 0.0134 - val_accuracy: 0.6638\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 54s 432ms/step - loss: 0.0220 - accuracy: 0.6986 - val_loss: 0.0209 - val_accuracy: 0.6638\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0206 - accuracy: 0.6986 - val_loss: 0.0168 - val_accuracy: 0.6638\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 53s 422ms/step - loss: 0.0185 - accuracy: 0.6986 - val_loss: 0.0169 - val_accuracy: 0.6638\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 54s 433ms/step - loss: 0.0177 - accuracy: 0.6986 - val_loss: 0.0147 - val_accuracy: 0.6638\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 52s 419ms/step - loss: 0.0166 - accuracy: 0.6986 - val_loss: 0.0142 - val_accuracy: 0.6638\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 53s 428ms/step - loss: 0.0157 - accuracy: 0.6986 - val_loss: 0.0151 - val_accuracy: 0.6638\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 53s 425ms/step - loss: 0.0148 - accuracy: 0.6986 - val_loss: 0.0116 - val_accuracy: 0.6638\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 52s 420ms/step - loss: 0.0141 - accuracy: 0.6986 - val_loss: 0.0121 - val_accuracy: 0.6638\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 53s 422ms/step - loss: 0.0132 - accuracy: 0.6986 - val_loss: 0.0109 - val_accuracy: 0.6638\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 52s 419ms/step - loss: 0.0126 - accuracy: 0.6986 - val_loss: 0.0118 - val_accuracy: 0.6638\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0122 - accuracy: 0.6986 - val_loss: 0.0133 - val_accuracy: 0.6638\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 52s 417ms/step - loss: 0.0115 - accuracy: 0.6986 - val_loss: 0.0109 - val_accuracy: 0.6638\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 52s 419ms/step - loss: 0.0110 - accuracy: 0.6986 - val_loss: 0.0134 - val_accuracy: 0.6638\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0107 - accuracy: 0.6986 - val_loss: 0.0093 - val_accuracy: 0.6638\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 52s 419ms/step - loss: 0.0102 - accuracy: 0.6986 - val_loss: 0.0106 - val_accuracy: 0.6638\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 52s 413ms/step - loss: 0.0099 - accuracy: 0.6986 - val_loss: 0.0088 - val_accuracy: 0.6638\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 52s 420ms/step - loss: 0.0096 - accuracy: 0.6986 - val_loss: 0.0113 - val_accuracy: 0.6638\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 53s 424ms/step - loss: 0.0093 - accuracy: 0.6986 - val_loss: 0.0091 - val_accuracy: 0.6638\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 53s 420ms/step - loss: 0.0090 - accuracy: 0.6986 - val_loss: 0.0104 - val_accuracy: 0.6638\n",
            "Epoch 28/100\n",
            "125/125 [==============================] - 53s 426ms/step - loss: 0.0086 - accuracy: 0.6986 - val_loss: 0.0089 - val_accuracy: 0.6638\n",
            "Epoch 29/100\n",
            "125/125 [==============================] - 53s 428ms/step - loss: 0.0084 - accuracy: 0.6986 - val_loss: 0.0085 - val_accuracy: 0.6638\n",
            "Epoch 30/100\n",
            "125/125 [==============================] - 54s 432ms/step - loss: 0.0083 - accuracy: 0.6986 - val_loss: 0.0071 - val_accuracy: 0.6638\n",
            "Epoch 31/100\n",
            "125/125 [==============================] - 54s 431ms/step - loss: 0.0080 - accuracy: 0.6986 - val_loss: 0.0080 - val_accuracy: 0.6638\n",
            "Epoch 32/100\n",
            "125/125 [==============================] - 54s 429ms/step - loss: 0.0078 - accuracy: 0.6986 - val_loss: 0.0061 - val_accuracy: 0.6638\n",
            "Epoch 33/100\n",
            "125/125 [==============================] - 54s 430ms/step - loss: 0.0075 - accuracy: 0.6986 - val_loss: 0.0071 - val_accuracy: 0.6638\n",
            "Epoch 34/100\n",
            "125/125 [==============================] - 53s 424ms/step - loss: 0.0074 - accuracy: 0.6986 - val_loss: 0.0077 - val_accuracy: 0.6638\n",
            "Epoch 35/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0072 - accuracy: 0.6986 - val_loss: 0.0063 - val_accuracy: 0.6638\n",
            "Epoch 36/100\n",
            "125/125 [==============================] - 53s 422ms/step - loss: 0.0069 - accuracy: 0.6986 - val_loss: 0.0062 - val_accuracy: 0.6638\n",
            "Epoch 37/100\n",
            "125/125 [==============================] - 52s 416ms/step - loss: 0.0068 - accuracy: 0.6986 - val_loss: 0.0059 - val_accuracy: 0.6638\n",
            "Epoch 38/100\n",
            "125/125 [==============================] - 52s 417ms/step - loss: 0.0066 - accuracy: 0.6986 - val_loss: 0.0068 - val_accuracy: 0.6638\n",
            "Epoch 39/100\n",
            "125/125 [==============================] - 52s 417ms/step - loss: 0.0063 - accuracy: 0.6986 - val_loss: 0.0063 - val_accuracy: 0.6638\n",
            "Epoch 40/100\n",
            "125/125 [==============================] - 52s 418ms/step - loss: 0.0062 - accuracy: 0.6986 - val_loss: 0.0054 - val_accuracy: 0.6638\n",
            "Epoch 41/100\n",
            "125/125 [==============================] - 53s 427ms/step - loss: 0.0060 - accuracy: 0.6986 - val_loss: 0.0059 - val_accuracy: 0.6638\n",
            "Epoch 42/100\n",
            "125/125 [==============================] - 53s 427ms/step - loss: 0.0057 - accuracy: 0.6986 - val_loss: 0.0060 - val_accuracy: 0.6638\n",
            "Epoch 43/100\n",
            "125/125 [==============================] - 53s 420ms/step - loss: 0.0056 - accuracy: 0.6986 - val_loss: 0.0046 - val_accuracy: 0.6638\n",
            "Epoch 44/100\n",
            "125/125 [==============================] - 52s 418ms/step - loss: 0.0054 - accuracy: 0.6986 - val_loss: 0.0038 - val_accuracy: 0.6638\n",
            "Epoch 45/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0052 - accuracy: 0.6986 - val_loss: 0.0070 - val_accuracy: 0.6638\n",
            "Epoch 46/100\n",
            "125/125 [==============================] - 52s 417ms/step - loss: 0.0051 - accuracy: 0.6986 - val_loss: 0.0054 - val_accuracy: 0.6638\n",
            "Epoch 47/100\n",
            "125/125 [==============================] - 52s 419ms/step - loss: 0.0050 - accuracy: 0.6986 - val_loss: 0.0040 - val_accuracy: 0.6638\n",
            "Epoch 48/100\n",
            "125/125 [==============================] - 53s 425ms/step - loss: 0.0048 - accuracy: 0.6986 - val_loss: 0.0047 - val_accuracy: 0.6638\n",
            "Epoch 49/100\n",
            "125/125 [==============================] - 52s 418ms/step - loss: 0.0047 - accuracy: 0.6986 - val_loss: 0.0042 - val_accuracy: 0.6638\n",
            "Epoch 50/100\n",
            "125/125 [==============================] - 51s 412ms/step - loss: 0.0046 - accuracy: 0.6986 - val_loss: 0.0045 - val_accuracy: 0.6638\n",
            "Epoch 51/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0045 - accuracy: 0.6986 - val_loss: 0.0045 - val_accuracy: 0.6638\n",
            "Epoch 52/100\n",
            "125/125 [==============================] - 52s 420ms/step - loss: 0.0043 - accuracy: 0.6986 - val_loss: 0.0044 - val_accuracy: 0.6638\n",
            "Epoch 53/100\n",
            "125/125 [==============================] - 54s 430ms/step - loss: 0.0042 - accuracy: 0.6986 - val_loss: 0.0043 - val_accuracy: 0.6638\n",
            "Epoch 54/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0042 - accuracy: 0.6986 - val_loss: 0.0038 - val_accuracy: 0.6638\n",
            "Epoch 55/100\n",
            "125/125 [==============================] - 52s 418ms/step - loss: 0.0041 - accuracy: 0.6986 - val_loss: 0.0039 - val_accuracy: 0.6638\n",
            "Epoch 56/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0040 - accuracy: 0.6986 - val_loss: 0.0037 - val_accuracy: 0.6638\n",
            "Epoch 57/100\n",
            "125/125 [==============================] - 53s 423ms/step - loss: 0.0039 - accuracy: 0.6986 - val_loss: 0.0029 - val_accuracy: 0.6638\n",
            "Epoch 58/100\n",
            "125/125 [==============================] - 52s 418ms/step - loss: 0.0038 - accuracy: 0.6986 - val_loss: 0.0033 - val_accuracy: 0.6638\n",
            "Epoch 59/100\n",
            "125/125 [==============================] - 53s 420ms/step - loss: 0.0038 - accuracy: 0.6986 - val_loss: 0.0037 - val_accuracy: 0.6638\n",
            "Epoch 60/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0037 - accuracy: 0.6986 - val_loss: 0.0031 - val_accuracy: 0.6638\n",
            "Epoch 61/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0037 - accuracy: 0.6986 - val_loss: 0.0039 - val_accuracy: 0.6638\n",
            "Epoch 62/100\n",
            "125/125 [==============================] - 52s 417ms/step - loss: 0.0036 - accuracy: 0.6986 - val_loss: 0.0034 - val_accuracy: 0.6638\n",
            "Epoch 63/100\n",
            "125/125 [==============================] - 52s 417ms/step - loss: 0.0035 - accuracy: 0.6986 - val_loss: 0.0041 - val_accuracy: 0.6638\n",
            "Epoch 64/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0035 - accuracy: 0.6986 - val_loss: 0.0035 - val_accuracy: 0.6638\n",
            "Epoch 65/100\n",
            "125/125 [==============================] - 53s 427ms/step - loss: 0.0034 - accuracy: 0.6986 - val_loss: 0.0031 - val_accuracy: 0.6638\n",
            "Epoch 66/100\n",
            "125/125 [==============================] - 52s 416ms/step - loss: 0.0032 - accuracy: 0.6986 - val_loss: 0.0029 - val_accuracy: 0.6638\n",
            "Epoch 67/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0032 - accuracy: 0.6986 - val_loss: 0.0033 - val_accuracy: 0.6638\n",
            "Epoch 68/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0032 - accuracy: 0.6986 - val_loss: 0.0033 - val_accuracy: 0.6638\n",
            "Epoch 69/100\n",
            "125/125 [==============================] - 53s 423ms/step - loss: 0.0031 - accuracy: 0.6986 - val_loss: 0.0033 - val_accuracy: 0.6638\n",
            "Epoch 70/100\n",
            "125/125 [==============================] - 52s 418ms/step - loss: 0.0031 - accuracy: 0.6986 - val_loss: 0.0030 - val_accuracy: 0.6638\n",
            "Epoch 71/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0030 - accuracy: 0.6986 - val_loss: 0.0026 - val_accuracy: 0.6638\n",
            "Epoch 72/100\n",
            "125/125 [==============================] - 52s 420ms/step - loss: 0.0029 - accuracy: 0.6986 - val_loss: 0.0032 - val_accuracy: 0.6638\n",
            "Epoch 73/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0029 - accuracy: 0.6986 - val_loss: 0.0028 - val_accuracy: 0.6638\n",
            "Epoch 74/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0029 - accuracy: 0.6986 - val_loss: 0.0024 - val_accuracy: 0.6638\n",
            "Epoch 75/100\n",
            "125/125 [==============================] - 52s 419ms/step - loss: 0.0028 - accuracy: 0.6986 - val_loss: 0.0027 - val_accuracy: 0.6638\n",
            "Epoch 76/100\n",
            "125/125 [==============================] - 52s 414ms/step - loss: 0.0028 - accuracy: 0.6986 - val_loss: 0.0026 - val_accuracy: 0.6638\n",
            "Epoch 77/100\n",
            "125/125 [==============================] - 52s 416ms/step - loss: 0.0026 - accuracy: 0.6986 - val_loss: 0.0037 - val_accuracy: 0.6638\n",
            "Epoch 78/100\n",
            "125/125 [==============================] - 53s 423ms/step - loss: 0.0026 - accuracy: 0.6986 - val_loss: 0.0024 - val_accuracy: 0.6638\n",
            "Epoch 79/100\n",
            "125/125 [==============================] - 52s 414ms/step - loss: 0.0026 - accuracy: 0.6986 - val_loss: 0.0020 - val_accuracy: 0.6638\n",
            "Epoch 80/100\n",
            "125/125 [==============================] - 52s 418ms/step - loss: 0.0025 - accuracy: 0.6986 - val_loss: 0.0027 - val_accuracy: 0.6638\n",
            "Epoch 81/100\n",
            "125/125 [==============================] - 54s 431ms/step - loss: 0.0024 - accuracy: 0.6986 - val_loss: 0.0020 - val_accuracy: 0.6638\n",
            "Epoch 82/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0024 - accuracy: 0.6986 - val_loss: 0.0023 - val_accuracy: 0.6638\n",
            "Epoch 83/100\n",
            "125/125 [==============================] - 52s 417ms/step - loss: 0.0024 - accuracy: 0.6986 - val_loss: 0.0022 - val_accuracy: 0.6638\n",
            "Epoch 84/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0023 - accuracy: 0.6986 - val_loss: 0.0025 - val_accuracy: 0.6638\n",
            "Epoch 85/100\n",
            "125/125 [==============================] - 52s 418ms/step - loss: 0.0023 - accuracy: 0.6986 - val_loss: 0.0018 - val_accuracy: 0.6638\n",
            "Epoch 86/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0022 - accuracy: 0.6986 - val_loss: 0.0021 - val_accuracy: 0.6638\n",
            "Epoch 87/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0022 - accuracy: 0.6986 - val_loss: 0.0021 - val_accuracy: 0.6638\n",
            "Epoch 88/100\n",
            "125/125 [==============================] - 51s 411ms/step - loss: 0.0022 - accuracy: 0.6986 - val_loss: 0.0018 - val_accuracy: 0.6638\n",
            "Epoch 89/100\n",
            "125/125 [==============================] - 52s 417ms/step - loss: 0.0021 - accuracy: 0.6986 - val_loss: 0.0021 - val_accuracy: 0.6638\n",
            "Epoch 90/100\n",
            "125/125 [==============================] - 52s 415ms/step - loss: 0.0021 - accuracy: 0.6986 - val_loss: 0.0021 - val_accuracy: 0.6638\n",
            "Epoch 91/100\n",
            "125/125 [==============================] - 52s 420ms/step - loss: 0.0021 - accuracy: 0.6986 - val_loss: 0.0022 - val_accuracy: 0.6638\n",
            "Epoch 92/100\n",
            "125/125 [==============================] - 52s 416ms/step - loss: 0.0021 - accuracy: 0.6986 - val_loss: 0.0016 - val_accuracy: 0.6638\n",
            "Epoch 93/100\n",
            "125/125 [==============================] - 53s 427ms/step - loss: 0.0020 - accuracy: 0.6986 - val_loss: 0.0023 - val_accuracy: 0.6638\n",
            "Epoch 94/100\n",
            "125/125 [==============================] - 52s 420ms/step - loss: 0.0020 - accuracy: 0.6986 - val_loss: 0.0019 - val_accuracy: 0.6638\n",
            "Epoch 95/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0019 - accuracy: 0.6986 - val_loss: 0.0026 - val_accuracy: 0.6638\n",
            "Epoch 96/100\n",
            "125/125 [==============================] - 52s 417ms/step - loss: 0.0019 - accuracy: 0.6986 - val_loss: 0.0026 - val_accuracy: 0.6638\n",
            "Epoch 97/100\n",
            "125/125 [==============================] - 51s 411ms/step - loss: 0.0018 - accuracy: 0.6986 - val_loss: 0.0018 - val_accuracy: 0.6638\n",
            "Epoch 98/100\n",
            "125/125 [==============================] - 52s 414ms/step - loss: 0.0018 - accuracy: 0.6986 - val_loss: 0.0018 - val_accuracy: 0.6638\n",
            "Epoch 99/100\n",
            "125/125 [==============================] - 53s 425ms/step - loss: 0.0018 - accuracy: 0.6986 - val_loss: 0.0018 - val_accuracy: 0.6638\n",
            "Epoch 100/100\n",
            "125/125 [==============================] - 53s 421ms/step - loss: 0.0017 - accuracy: 0.6986 - val_loss: 0.0016 - val_accuracy: 0.6638\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fad1ec00ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rFNPigLn6i4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Seq2seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}